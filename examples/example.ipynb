{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0517ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from src.topicbench.label_topics import label_topics\n",
    "from src.topicbench.validate import score_similarity, compute_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eaf833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the models that will be benchmarked\n",
    "models_config = {\n",
    "    'llama3.2:latest': {'api_key_path': None, 'type': 'local'},\n",
    "    # 'gpt-oss:20b': {'api_key_path': None, 'type': 'local'},\n",
    "    # 'gpt-3.5-turbo': {'api_key_path': '../../openai_key.txt', 'type': 'api'},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ca38ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TopicBench dataset\n",
    "df = pd.read_csv('../data/data_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "741ad1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing with llama3.2:latest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling topics with llama3.2:latest: 100%|██████████| 5/5 [00:47<00:00,  9.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed llama3.2:latest\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Label topics using each model and store results\n",
    "for model_name, config in models_config.items():\n",
    "    print(f\"Processing with {model_name}...\")\n",
    "    results_df = label_topics(df, model_name=model_name, API_KEY_PATH=config['api_key_path'])\n",
    "    print(f\"Completed {model_name}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244f1dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing similarity for llama3.2:latest: 100%|██████████| 5/5 [00:14<00:00,  2.83s/it]\n",
      "Computing similarity for alt_human: 100%|██████████| 5/5 [00:14<00:00,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity computations complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute similarity scores for author vs alt_human and author vs each AI model\n",
    "\n",
    "# Define the metric to use\n",
    "metric = 'cosine'\n",
    "\n",
    "def safe_parse_labels(label_string):\n",
    "    \"\"\"Safely parse label strings that may contain apostrophes or other special characters.\"\"\"\n",
    "    try:\n",
    "        # First try ast.literal_eval\n",
    "        return ast.literal_eval(label_string)\n",
    "    except (ValueError, SyntaxError):\n",
    "        try:\n",
    "            # If that fails, try json.loads (need to replace single quotes with double quotes)\n",
    "            return json.loads(label_string.replace(\"'\", '\"'))\n",
    "        except json.JSONDecodeError:\n",
    "            # If both fail, return the string as-is wrapped in a list\n",
    "            print(f\"Warning: Could not parse: {label_string[:100]}\")\n",
    "            return [label_string]\n",
    "\n",
    "# For each model in the config, compute similarity scores\n",
    "for model_name in models_config.keys():\n",
    "    all_scores = []\n",
    "    \n",
    "    for idx in tqdm(range(len(results_df)), desc=f\"Computing similarity for {model_name}\"):\n",
    "        row = results_df.iloc[idx]\n",
    "        \n",
    "        # Parse the label lists (they are stored as strings)\n",
    "        author_labels = safe_parse_labels(row['author_label'])\n",
    "        comparison_labels = safe_parse_labels(row[model_name])\n",
    "        \n",
    "        # Compute pairwise similarities between corresponding labels\n",
    "        # Each topic's label is compared to the corresponding author label\n",
    "        label_similarities = []\n",
    "        for author_label, comp_label in zip(author_labels, comparison_labels):\n",
    "            score = score_similarity(author_label, comp_label, metric=metric)\n",
    "            label_similarities.append(float(score))\n",
    "        \n",
    "        # Store the list of scores for this row\n",
    "        all_scores.append(label_similarities)\n",
    "    \n",
    "    # Add scores as a new column with format: model_name_metric_scores\n",
    "    results_df[f'{model_name}_{metric}_similarity'] = all_scores\n",
    "\n",
    "# Also compute for alt_human\n",
    "alt_human_scores = []\n",
    "for idx in tqdm(range(len(results_df)), desc=\"Computing similarity for alt_human\"):\n",
    "    row = results_df.iloc[idx]\n",
    "    \n",
    "    # Parse the label lists\n",
    "    author_labels = safe_parse_labels(row['author_label'])\n",
    "    alt_human_labels = safe_parse_labels(row['alt_human'])\n",
    "    \n",
    "    # Compute pairwise similarities\n",
    "    label_similarities = []\n",
    "    for author_label, alt_label in zip(author_labels, alt_human_labels):\n",
    "        score = score_similarity(author_label, alt_label, metric=metric)\n",
    "        label_similarities.append(float(score))\n",
    "    \n",
    "    alt_human_scores.append(label_similarities)\n",
    "\n",
    "results_df[f'alt_human_{metric}_similarity'] = alt_human_scores\n",
    "\n",
    "print(\"Similarity computations complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9355b581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_title</th>\n",
       "      <th>field</th>\n",
       "      <th>keywords</th>\n",
       "      <th>author_label</th>\n",
       "      <th>alt_human</th>\n",
       "      <th>llama3.2:latest</th>\n",
       "      <th>llama3.2:latest_cosine_similarity</th>\n",
       "      <th>alt_human_cosine_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>almquist_bagozzi_2019</td>\n",
       "      <td>sociology</td>\n",
       "      <td>[['one', 'made', 'anoth', 'everi', 'side', 'ti...</td>\n",
       "      <td>['Inspirational Language', 'Group Identity Deb...</td>\n",
       "      <td>['Motivational Rhetoric', 'Collective Identity...</td>\n",
       "      <td>['Social Movement', 'Activism', 'Protest', 'Re...</td>\n",
       "      <td>[0.22654280066490173, 0.4005390405654907, 0.34...</td>\n",
       "      <td>[0.4601529836654663, 0.857661247253418, 0.7370...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dinsa_2024</td>\n",
       "      <td>medicine</td>\n",
       "      <td>[['body', 'came', 'dries up', 'rocking', 'time...</td>\n",
       "      <td>['Nervous disease', 'Gynecology', 'Mental illn...</td>\n",
       "      <td>['Fatigue &amp; General Body Weakness', 'Pregnancy...</td>\n",
       "      <td>['Gastrointestinal Issues', 'Menstrual Cycle',...</td>\n",
       "      <td>[0.15294265747070312, 0.41162168979644775, 0.3...</td>\n",
       "      <td>[0.2469634711742401, 0.31719857454299927, 0.11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>farrell_2015</td>\n",
       "      <td>environmental science</td>\n",
       "      <td>[['one', 'will', 'peopl', 'can', 'just', 'get'...</td>\n",
       "      <td>[\"People's Knowledge\", 'Skeptical of IPCC Scie...</td>\n",
       "      <td>['General Discourse', 'Climate Science &amp; IPCC ...</td>\n",
       "      <td>['Environmental Impact', 'Climate Change', 'Gl...</td>\n",
       "      <td>[0.14470866322517395, 0.19482237100601196, 0.4...</td>\n",
       "      <td>[0.28594285249710083, 0.7122806906700134, 0.35...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>liao_2022</td>\n",
       "      <td>hci</td>\n",
       "      <td>[['can', 'get', 'us', 'cant', 'work', 'please'...</td>\n",
       "      <td>['Problem solving', 'Desperate effort to log i...</td>\n",
       "      <td>['Technical Support &amp; DNS Issues', 'App Login ...</td>\n",
       "      <td>['error', 'problem', 'network', 'server', 'tes...</td>\n",
       "      <td>[0.2958523631095886, 0.28478139638900757, 0.13...</td>\n",
       "      <td>[0.21905887126922607, 0.4882417917251587, 0.13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>paul_girju_2009</td>\n",
       "      <td>nlp</td>\n",
       "      <td>[['pragmatics', 'attitudes', 'meaning', 'seman...</td>\n",
       "      <td>['Pragmatics', 'Prosody', 'Psycholinguistics',...</td>\n",
       "      <td>['Pragmatics &amp; Communicative Inference', 'Pros...</td>\n",
       "      <td>['pragmatics', 'attitudes', 'meaning', 'semant...</td>\n",
       "      <td>[1.0000001192092896, 0.22162874042987823, 0.26...</td>\n",
       "      <td>[0.766014575958252, 0.8201093673706055, 0.8852...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             paper_title                  field  \\\n",
       "0  almquist_bagozzi_2019              sociology   \n",
       "1             dinsa_2024               medicine   \n",
       "2           farrell_2015  environmental science   \n",
       "3              liao_2022                    hci   \n",
       "4        paul_girju_2009                    nlp   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  [['one', 'made', 'anoth', 'everi', 'side', 'ti...   \n",
       "1  [['body', 'came', 'dries up', 'rocking', 'time...   \n",
       "2  [['one', 'will', 'peopl', 'can', 'just', 'get'...   \n",
       "3  [['can', 'get', 'us', 'cant', 'work', 'please'...   \n",
       "4  [['pragmatics', 'attitudes', 'meaning', 'seman...   \n",
       "\n",
       "                                        author_label  \\\n",
       "0  ['Inspirational Language', 'Group Identity Deb...   \n",
       "1  ['Nervous disease', 'Gynecology', 'Mental illn...   \n",
       "2  [\"People's Knowledge\", 'Skeptical of IPCC Scie...   \n",
       "3  ['Problem solving', 'Desperate effort to log i...   \n",
       "4  ['Pragmatics', 'Prosody', 'Psycholinguistics',...   \n",
       "\n",
       "                                           alt_human  \\\n",
       "0  ['Motivational Rhetoric', 'Collective Identity...   \n",
       "1  ['Fatigue & General Body Weakness', 'Pregnancy...   \n",
       "2  ['General Discourse', 'Climate Science & IPCC ...   \n",
       "3  ['Technical Support & DNS Issues', 'App Login ...   \n",
       "4  ['Pragmatics & Communicative Inference', 'Pros...   \n",
       "\n",
       "                                     llama3.2:latest  \\\n",
       "0  ['Social Movement', 'Activism', 'Protest', 'Re...   \n",
       "1  ['Gastrointestinal Issues', 'Menstrual Cycle',...   \n",
       "2  ['Environmental Impact', 'Climate Change', 'Gl...   \n",
       "3  ['error', 'problem', 'network', 'server', 'tes...   \n",
       "4  ['pragmatics', 'attitudes', 'meaning', 'semant...   \n",
       "\n",
       "                   llama3.2:latest_cosine_similarity  \\\n",
       "0  [0.22654280066490173, 0.4005390405654907, 0.34...   \n",
       "1  [0.15294265747070312, 0.41162168979644775, 0.3...   \n",
       "2  [0.14470866322517395, 0.19482237100601196, 0.4...   \n",
       "3  [0.2958523631095886, 0.28478139638900757, 0.13...   \n",
       "4  [1.0000001192092896, 0.22162874042987823, 0.26...   \n",
       "\n",
       "                         alt_human_cosine_similarity  \n",
       "0  [0.4601529836654663, 0.857661247253418, 0.7370...  \n",
       "1  [0.2469634711742401, 0.31719857454299927, 0.11...  \n",
       "2  [0.28594285249710083, 0.7122806906700134, 0.35...  \n",
       "3  [0.21905887126922607, 0.4882417917251587, 0.13...  \n",
       "4  [0.766014575958252, 0.8201093673706055, 0.8852...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2c42a0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply compute_alignment element-wise to each score in the lists for all models\n",
    "for model_name in models_config.keys():\n",
    "    alignment_scores = []\n",
    "    tau_values = []\n",
    "    \n",
    "    for idx, row in results_df.iterrows():\n",
    "        human_scores = row['alt_human_cosine_similarity']\n",
    "        ai_scores = row[f'{model_name}_cosine_similarity']\n",
    "        \n",
    "        # Ensure both are lists and have the same length\n",
    "        if not isinstance(human_scores, list):\n",
    "            human_scores = list(human_scores) if hasattr(human_scores, '__iter__') else [human_scores]\n",
    "        if not isinstance(ai_scores, list):\n",
    "            ai_scores = list(ai_scores) if hasattr(ai_scores, '__iter__') else [ai_scores]\n",
    "        \n",
    "        # Handle potential length mismatch\n",
    "        min_len = min(len(human_scores), len(ai_scores))\n",
    "        human_scores = human_scores[:min_len]\n",
    "        ai_scores = ai_scores[:min_len]\n",
    "        \n",
    "        # Create a temporary dataframe with the individual scores\n",
    "        temp_df = pd.DataFrame({\n",
    "            'human_similarity': human_scores,\n",
    "            'ai_similarity': ai_scores\n",
    "        })\n",
    "        \n",
    "        # Apply compute_alignment to this temporary dataframe\n",
    "        aligned_temp = compute_alignment(temp_df, human_col='human_similarity', ai_col='ai_similarity', tau=0)\n",
    "        \n",
    "        # Extract the alignment results and tau value as lists\n",
    "        alignment_list = aligned_temp['AI_alignment'].tolist()\n",
    "        tau_value = aligned_temp['tau'].iloc[0]\n",
    "        \n",
    "        alignment_scores.append(alignment_list)\n",
    "        tau_values.append(tau_value)\n",
    "    \n",
    "    # Add the alignment scores and tau values to your results dataframe with model-specific names\n",
    "    results_df[f'{model_name}_alignment_scores'] = alignment_scores\n",
    "    results_df[f'{model_name}_tau_values'] = tau_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aeae6e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of alignment scores for each row for all models\n",
    "for model_name in models_config.keys():\n",
    "    results_df[f'{model_name}_final_score'] = results_df[f'{model_name}_alignment_scores'].apply(lambda x: np.mean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9f4faed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field</th>\n",
       "      <th>author_label</th>\n",
       "      <th>alt_human</th>\n",
       "      <th>llama3.2:latest</th>\n",
       "      <th>gpt-3.5-turbo</th>\n",
       "      <th>llama3.2:latest_final_score</th>\n",
       "      <th>gpt-3.5-turbo_final_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sociology</td>\n",
       "      <td>['Inspirational Language', 'Group Identity Deb...</td>\n",
       "      <td>['Motivational Rhetoric', 'Collective Identity...</td>\n",
       "      <td>['Social Movement', 'Activism', 'Resistance', ...</td>\n",
       "      <td>['movement and activism', 'environmental conse...</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>medicine</td>\n",
       "      <td>['Nervous disease', 'Gynecology', 'Mental illn...</td>\n",
       "      <td>['Fatigue &amp; General Body Weakness', 'Pregnancy...</td>\n",
       "      <td>['Gastrointestinal issues', 'Pregnancy complic...</td>\n",
       "      <td>['Physical Symptoms', 'Pregnancy-related Issue...</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>environmental science</td>\n",
       "      <td>[\"People's Knowledge\", 'Skeptical of IPCC Scie...</td>\n",
       "      <td>['General Discourse', 'Climate Science &amp; IPCC ...</td>\n",
       "      <td>['Climate Change', 'Global Warming', 'Environm...</td>\n",
       "      <td>['general concepts', 'climate science research...</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.461538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hci</td>\n",
       "      <td>['Problem solving', 'Desperate effort to log i...</td>\n",
       "      <td>['Technical Support &amp; DNS Issues', 'App Login ...</td>\n",
       "      <td>['technical issues', 'network problems', 'dns ...</td>\n",
       "      <td>['technical support', 'app usability', 'social...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nlp</td>\n",
       "      <td>['Pragmatics', 'Prosody', 'Psycholinguistics',...</td>\n",
       "      <td>['Pragmatics &amp; Communicative Inference', 'Pros...</td>\n",
       "      <td>['pragmatics', 'semantics', 'inference', 'comm...</td>\n",
       "      <td>['pragmatics', 'semantics', 'communication', '...</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   field                                       author_label  \\\n",
       "0              sociology  ['Inspirational Language', 'Group Identity Deb...   \n",
       "1               medicine  ['Nervous disease', 'Gynecology', 'Mental illn...   \n",
       "2  environmental science  [\"People's Knowledge\", 'Skeptical of IPCC Scie...   \n",
       "3                    hci  ['Problem solving', 'Desperate effort to log i...   \n",
       "4                    nlp  ['Pragmatics', 'Prosody', 'Psycholinguistics',...   \n",
       "\n",
       "                                           alt_human  \\\n",
       "0  ['Motivational Rhetoric', 'Collective Identity...   \n",
       "1  ['Fatigue & General Body Weakness', 'Pregnancy...   \n",
       "2  ['General Discourse', 'Climate Science & IPCC ...   \n",
       "3  ['Technical Support & DNS Issues', 'App Login ...   \n",
       "4  ['Pragmatics & Communicative Inference', 'Pros...   \n",
       "\n",
       "                                     llama3.2:latest  \\\n",
       "0  ['Social Movement', 'Activism', 'Resistance', ...   \n",
       "1  ['Gastrointestinal issues', 'Pregnancy complic...   \n",
       "2  ['Climate Change', 'Global Warming', 'Environm...   \n",
       "3  ['technical issues', 'network problems', 'dns ...   \n",
       "4  ['pragmatics', 'semantics', 'inference', 'comm...   \n",
       "\n",
       "                                       gpt-3.5-turbo  \\\n",
       "0  ['movement and activism', 'environmental conse...   \n",
       "1  ['Physical Symptoms', 'Pregnancy-related Issue...   \n",
       "2  ['general concepts', 'climate science research...   \n",
       "3  ['technical support', 'app usability', 'social...   \n",
       "4  ['pragmatics', 'semantics', 'communication', '...   \n",
       "\n",
       "   llama3.2:latest_final_score  gpt-3.5-turbo_final_score  \n",
       "0                     0.600000                   0.333333  \n",
       "1                     0.222222                   0.200000  \n",
       "2                     0.576923                   0.461538  \n",
       "3                     0.400000                   0.300000  \n",
       "4                     0.266667                   0.400000  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display results for all models\n",
    "score_columns = [f'{model_name}_alignment_scores' for model_name in models_config.keys()] + \\\n",
    "                [f'{model_name}_final_score' for model_name in models_config.keys()]\n",
    "results_df[['field', 'author_label', 'alt_human', 'llama3.2:latest', 'gpt-3.5-turbo', 'llama3.2:latest_final_score', 'gpt-3.5-turbo_final_score']].head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topicbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
