{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f846e1f",
   "metadata": {},
   "source": [
    "# LLM As A Judge\n",
    "### Authored by Zilin Cheng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67706c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm  # optional progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6623e48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load CSV ---\n",
    "df = pd.read_csv('evaluation_cases.csv')\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795d7f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Settings ---\n",
    "MODEL = \"gpt-5\"\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key>\"))\n",
    "\n",
    "# --- Loop through pairs ---\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    sent1, sent2 = row[\"sent1\"], row[\"sent2\"]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Compare the following texts and rate their semantic similarity from 0 (orthogonal) to 1 (exact match). Provide a short reasoning before giving the score.\n",
    "\n",
    "Sentence 1: {sent1}\n",
    "Sentence 2: {sent2}\n",
    "\n",
    "Return your answer in strict JSON format:\n",
    "{{\n",
    "  \"reasoning\": \"<short explanation>\",\n",
    "  \"similarity_score\": <a number between 0 and 1>\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert semantic similarity judge.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"}  # ensures valid JSON\n",
    "    )\n",
    "\n",
    "    output = json.loads(response.choices[0].message.content)\n",
    "    output[\"sent1\"] = sent1\n",
    "    output[\"sent2\"] = sent2\n",
    "    results.append(output)\n",
    "\n",
    "# --- Save JSON output ---\n",
    "with open(\"sentence_similarity_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f92510a",
   "metadata": {},
   "source": [
    "What it is:\n",
    "Prompt a language model with Text A and Text B; return a 0â€“1 similarity score and a one-sentence rationale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e129156",
   "metadata": {},
   "source": [
    "Pros:\n",
    "\n",
    "1. Semantic fidelity: Best at paraphrase + world knowledge + negation handling\n",
    "\n",
    "2. Flexible calibration: We can define a rubric with anchor examples (0/0.25/0.5/0.75/1.0)\n",
    "\n",
    "3. Explainability: Short rationale increases trust\n",
    "\n",
    "Cons:\n",
    "\n",
    "1. Latency & cost: API calls; or local models need RAM/VRAM (Ollama)\n",
    "\n",
    "2. Stability: Prompt/model/version drift - requires guardrails\n",
    "\n",
    "3. Reproducibility: Even with the same model and input, LLMs can produce different results, and small prompt changes can shift outputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
