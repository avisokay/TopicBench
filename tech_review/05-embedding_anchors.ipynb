{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0574faa",
   "metadata": {},
   "source": [
    "# Local vs Global Semantics in Latent Space\n",
    "### Authored by Adam Visokay\n",
    "\n",
    "To make any similarity or distance metric globally interpretable, you need to anchor it to an empirical distribution that represents “the space of all language” (or at least, a large enough sample of it). Without such an anchor, the distances you compute in latent space are only meaningful in a local context, relative to the specific data points you are comparing.\n",
    "\n",
    "Without calibration, your scores are contextual — they only mean something relative to the other items in your dataset. For instance:\n",
    "\n",
    "- A cosine similarity of 0.75 could be “very high” in a corpus of random, unrelated sentences,\n",
    "\n",
    "- but “average” in a corpus of paraphrase pairs.\n",
    "\n",
    "So to make a similarity score global, you have to answer: “High relative to what?” That’s what a reference distribution provides.\n",
    "\n",
    "## Methods for Calibration - Brief Lit Review\n",
    "\n",
    "There are several methods and NLP papers that touch on the issues of embedding-space calibration, isotropy/anisotropy of embedding distributions, and to some extent the idea of grounding similarity scores in more global distributions.\n",
    "\n",
    "#### On the Sentence Embeddings from Pre-trained Language Models (Li et al., 2020)\n",
    "They show that embeddings from models like BERT without special processing tend to live in a narrow “cone” (i.e., anisotropic space), which hurts similarity tasks.\n",
    "**Links:** [arXiv](https://arxiv.org/abs/2012.14538)\n",
    "\n",
    "**Relevance:** This supports the claim that absolute distances/similarities are not straightforwardly calibrated because of the geometry of the embedding space.\n",
    "\n",
    "#### Whitening Sentence Representations for Better Semantics and Faster Retrieval (Su et al., 2021)\n",
    "**Summary:** They apply a “whitening” transformation to sentence embeddings (making mean zero, covariance ≈ identity) to improve retrieval and similarity tasks.  \n",
    "**Links:** [CatalyzeX](https://www.catalyzex.com/paper/arxiv:2103.15316)  \n",
    "\n",
    "**Relevance:** The whitening step effectively calibrates the embedding space — a key idea if you want global or interpretable similarity scores rather than dataset-relative ones.\n",
    "\n",
    "#### Semantic Alignment with Calibrated Similarity for Multilingual Sentence Embedding (Ham & Kim, 2021)\n",
    "**Summary:** Introduces a “calibrated similarity” for multilingual sentence embeddings. The goal is to align embeddings across languages in a globally comparable way.  \n",
    "**Links:** [ACL Anthology](https://aclanthology.org/2021.emnlp-main.381/)  \n",
    "\n",
    "**Relevance:** This provides direct precedent for introducing calibration of similarity metrics across large populations of embeddings — the idea of global rather than local similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cfcefc",
   "metadata": {},
   "source": [
    "## Borrowing from Hilary's work (`03-embedding_other.ipynb`) as an example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44af082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from scipy.spatial import distance\n",
    "from math import acos, pi\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dab5d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('evaluation_cases.csv')\n",
    "\n",
    "# load model (this might take a couple minutes the first time)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# create dataframe to store distances\n",
    "distances = pd.DataFrame(columns=['euclidean', 'manhattan', 'dot_product', 'angular'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae96d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the evaluation cases\n",
    "for index, row in df.iterrows():\n",
    "    text_a = row['sent1']\n",
    "    text_b = row['sent2']\n",
    "\n",
    "    # encode sentences\n",
    "    emb1 = model.encode(text_a, convert_to_tensor=True)\n",
    "    emb2 = model.encode(text_b, convert_to_tensor=True)\n",
    "\n",
    "    # compute distances\n",
    "    distances.at[index, 'euclidean'] = distance.euclidean(emb1.cpu().numpy(), emb2.cpu().numpy())\n",
    "    distances.at[index, 'manhattan'] = distance.cityblock(emb1.cpu().numpy(), emb2.cpu().numpy()) \n",
    "    distances.at[index, 'dot_product'] = torch.dot(emb1, emb2).item()\n",
    "    distances.at[index, 'angular'] = acos(1 - distance.cosine(emb1.cpu().numpy(), emb2.cpu().numpy()))/pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c8a188d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>euclidean</th>\n",
       "      <th>manhattan</th>\n",
       "      <th>dot_product</th>\n",
       "      <th>angular</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.935487</td>\n",
       "      <td>14.680563</td>\n",
       "      <td>0.562432</td>\n",
       "      <td>0.690134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.842431</td>\n",
       "      <td>13.078041</td>\n",
       "      <td>0.645155</td>\n",
       "      <td>0.723207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.41078</td>\n",
       "      <td>22.018671</td>\n",
       "      <td>0.004850</td>\n",
       "      <td>0.501544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  euclidean  manhattan  dot_product   angular\n",
       "0  0.935487  14.680563     0.562432  0.690134\n",
       "1       0.0        0.0     1.000000  1.000000\n",
       "2  0.842431  13.078041     0.645155  0.723207\n",
       "3   1.41078  22.018671     0.004850  0.501544"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d078ed8",
   "metadata": {},
   "source": [
    "# Option 1: Normalize distances using anchors\n",
    "\n",
    "To calibrate the distance metrics, I created arbitrary anchor pairs representing different levels of semantic similarity:\n",
    "- **Identical**: Same sentence (distance = 0)\n",
    "- **Paraphrase**: Semantically equivalent but different wording\n",
    "- **Related**: Topically related but different meaning\n",
    "- **Unrelated**: Completely different topics\n",
    "\n",
    "I'll use these anchors to normalize the distances into a 0-1 scale where:\n",
    "- 0 = identical (like our \"identical\" anchor)\n",
    "- 1 = maximally different (like our \"unrelated\" anchor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ndksir37g",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor distances:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>euclidean</th>\n",
       "      <th>manhattan</th>\n",
       "      <th>dot_product</th>\n",
       "      <th>angular</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>identical</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paraphrase</th>\n",
       "      <td>0.558132</td>\n",
       "      <td>8.714251</td>\n",
       "      <td>0.844245</td>\n",
       "      <td>0.819951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>related</th>\n",
       "      <td>1.010194</td>\n",
       "      <td>15.630564</td>\n",
       "      <td>0.489754</td>\n",
       "      <td>0.662913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unrelated</th>\n",
       "      <td>1.359313</td>\n",
       "      <td>20.842813</td>\n",
       "      <td>0.076134</td>\n",
       "      <td>0.524258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            euclidean  manhattan  dot_product   angular\n",
       "identical    0.000000   0.000000     1.000000  1.000000\n",
       "paraphrase   0.558132   8.714251     0.844245  0.819951\n",
       "related      1.010194  15.630564     0.489754  0.662913\n",
       "unrelated    1.359313  20.842813     0.076134  0.524258"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define anchor sentence pairs representing different levels of semantic similarity\n",
    "anchors = {\n",
    "    'identical': ('the quick brown fox jumps over the lazy dog', \n",
    "                  'the quick brown fox jumps over the lazy dog'),       \n",
    "    'paraphrase': ('the economy is growing rapidly', \n",
    "                   'economic growth is accelerating quickly'),\n",
    "    'related': ('the stock market crashed today', \n",
    "                'investors are worried about the economy'),\n",
    "    'unrelated': ('machine learning algorithms are improving', \n",
    "                  'the chef prepared a delicious pasta dish')\n",
    "}\n",
    "\n",
    "# Compute distances for anchor pairs\n",
    "anchor_distances = {}\n",
    "for label, (sent1, sent2) in anchors.items():\n",
    "    emb1 = model.encode(sent1, convert_to_tensor=True)\n",
    "    emb2 = model.encode(sent2, convert_to_tensor=True)\n",
    "    \n",
    "    anchor_distances[label] = {\n",
    "        'euclidean': distance.euclidean(emb1.cpu().numpy(), emb2.cpu().numpy()),\n",
    "        'manhattan': distance.cityblock(emb1.cpu().numpy(), emb2.cpu().numpy()),\n",
    "        'dot_product': torch.dot(emb1, emb2).item(),\n",
    "        'angular': acos(1 - distance.cosine(emb1.cpu().numpy(), emb2.cpu().numpy()))/pi\n",
    "    }\n",
    "\n",
    "# Display anchor distances\n",
    "anchor_df = pd.DataFrame(anchor_distances).T\n",
    "print(\"Anchor distances:\")\n",
    "anchor_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "g2x9r2cumha",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized distances (0 = identical, 1 = maximally different):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>euclidean_norm</th>\n",
       "      <th>manhattan_norm</th>\n",
       "      <th>angular_norm</th>\n",
       "      <th>dot_product_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.688206</td>\n",
       "      <td>0.704347</td>\n",
       "      <td>0.651331</td>\n",
       "      <td>4.736273e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-1.290332e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.619748</td>\n",
       "      <td>0.62746</td>\n",
       "      <td>0.581812</td>\n",
       "      <td>3.840870e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.037863</td>\n",
       "      <td>1.056415</td>\n",
       "      <td>1.047745</td>\n",
       "      <td>1.077159e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  euclidean_norm manhattan_norm  angular_norm  dot_product_norm\n",
       "0       0.688206       0.704347      0.651331      4.736273e-01\n",
       "1            0.0            0.0     -0.000000     -1.290332e-07\n",
       "2       0.619748        0.62746      0.581812      3.840870e-01\n",
       "3       1.037863       1.056415      1.047745      1.077159e+00"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create normalized distances using min-max normalization\n",
    "# We use 'identical' as min (0) and 'unrelated' as max (1)\n",
    "\n",
    "distances_norm = pd.DataFrame(index=distances.index)\n",
    "\n",
    "for metric in ['euclidean', 'manhattan', 'angular']:\n",
    "    min_val = anchor_distances['identical'][metric]\n",
    "    max_val = anchor_distances['unrelated'][metric]\n",
    "    \n",
    "    # Min-max normalization: (x - min) / (max - min)\n",
    "    distances_norm[f'{metric}_norm'] = (distances[metric] - min_val) / (max_val - min_val)\n",
    "\n",
    "# For dot_product, higher is more similar, so we need to invert\n",
    "# Normalize using max (identical) and min (unrelated)\n",
    "max_val = anchor_distances['identical']['dot_product']\n",
    "min_val = anchor_distances['unrelated']['dot_product']\n",
    "distances_norm['dot_product_norm'] = (max_val - distances['dot_product']) / (max_val - min_val)\n",
    "\n",
    "print(\"Normalized distances (0 = identical, 1 = maximally different):\")\n",
    "distances_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2azih74p4wy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>euclidean</th>\n",
       "      <th>euclidean_norm</th>\n",
       "      <th>manhattan</th>\n",
       "      <th>manhattan_norm</th>\n",
       "      <th>dot_product</th>\n",
       "      <th>dot_product_norm</th>\n",
       "      <th>angular</th>\n",
       "      <th>angular_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.935487</td>\n",
       "      <td>0.688206</td>\n",
       "      <td>14.680563</td>\n",
       "      <td>0.704347</td>\n",
       "      <td>0.562432</td>\n",
       "      <td>4.736273e-01</td>\n",
       "      <td>0.690134</td>\n",
       "      <td>0.651331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.290332e-07</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.842431</td>\n",
       "      <td>0.619748</td>\n",
       "      <td>13.078041</td>\n",
       "      <td>0.62746</td>\n",
       "      <td>0.645155</td>\n",
       "      <td>3.840870e-01</td>\n",
       "      <td>0.723207</td>\n",
       "      <td>0.581812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.41078</td>\n",
       "      <td>1.037863</td>\n",
       "      <td>22.018671</td>\n",
       "      <td>1.056415</td>\n",
       "      <td>0.004850</td>\n",
       "      <td>1.077159e+00</td>\n",
       "      <td>0.501544</td>\n",
       "      <td>1.047745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  euclidean euclidean_norm  manhattan manhattan_norm  dot_product  \\\n",
       "0  0.935487       0.688206  14.680563       0.704347     0.562432   \n",
       "1       0.0            0.0        0.0            0.0     1.000000   \n",
       "2  0.842431       0.619748  13.078041        0.62746     0.645155   \n",
       "3   1.41078       1.037863  22.018671       1.056415     0.004850   \n",
       "\n",
       "   dot_product_norm   angular  angular_norm  \n",
       "0      4.736273e-01  0.690134      0.651331  \n",
       "1     -1.290332e-07  1.000000     -0.000000  \n",
       "2      3.840870e-01  0.723207      0.581812  \n",
       "3      1.077159e+00  0.501544      1.047745  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine and reorder columns so raw and normalized metrics are side by side\n",
    "combined = pd.concat([distances, distances_norm], axis=1)\n",
    "\n",
    "# Reorder columns to group raw and normalized versions together\n",
    "column_order = [\n",
    "    'euclidean', 'euclidean_norm',\n",
    "    'manhattan', 'manhattan_norm',\n",
    "    'dot_product', 'dot_product_norm',\n",
    "    'angular', 'angular_norm'\n",
    "]\n",
    "\n",
    "combined = combined[column_order]\n",
    "combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a589b229",
   "metadata": {},
   "source": [
    "Including manually curated extremes helps ground the distances in a more interpretable way. But its kinda arbitrary and ad-hoc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ed6a3f",
   "metadata": {},
   "source": [
    "## Option 2: Background calibration with data-driven global baseline\n",
    "\n",
    "If you want something statistically grounded instead of manually curated, use a background corpus. Here I am using random pairs of sentences from the AG news dataset as a background distribution of “typical” distances. This will let me compare my target pairs against a more global baseline.\n",
    "\n",
    "<u>Note: In practice, you would want a much larger sample of background pairs (e.g., 1000s) to get a stable estimate of the distribution. Here I am just using a small sample for demonstration purposes. Also, because the cases I have include exact literal string matches, the background examples here are unlikely to include any identical pairs, so the min distance will be > 0. This will affect the normalization slightly and lead to some negative normalized distances for very similar pairs in my target set. In a real application, you would want to ensure your background corpus is large and diverse enough to include a full range of similarities, including identical pairs if possible.</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0641f48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing distances for 50 random sentence pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 13.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Background distribution statistics:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>euclidean</th>\n",
       "      <td>1.327357</td>\n",
       "      <td>0.106252</td>\n",
       "      <td>0.828798</td>\n",
       "      <td>1.468788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manhattan</th>\n",
       "      <td>20.643732</td>\n",
       "      <td>1.642879</td>\n",
       "      <td>12.957010</td>\n",
       "      <td>23.467381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dot_product</th>\n",
       "      <td>0.113417</td>\n",
       "      <td>0.127836</td>\n",
       "      <td>-0.078670</td>\n",
       "      <td>0.656547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>angular</th>\n",
       "      <td>0.536780</td>\n",
       "      <td>0.042826</td>\n",
       "      <td>0.474933</td>\n",
       "      <td>0.727984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  mean       std        min        max\n",
       "euclidean     1.327357  0.106252   0.828798   1.468788\n",
       "manhattan    20.643732  1.642879  12.957010  23.467381\n",
       "dot_product   0.113417  0.127836  -0.078670   0.656547\n",
       "angular       0.536780  0.042826   0.474933   0.727984"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using random sentences from AG News - estimate background distribution of distances\n",
    "news = load_dataset(\"ag_news\", split=\"train[:1000]\")\n",
    "sentences = [s['text'] for s in news if len(s['text'].split()) > 5]\n",
    "random_pairs = random.sample(list(zip(sentences[:-1], sentences[1:])), 50)\n",
    "\n",
    "# Compute all distance metrics for random pairs\n",
    "bg_distances = {\n",
    "    'euclidean': [],\n",
    "    'manhattan': [],\n",
    "    'dot_product': [],\n",
    "    'angular': []\n",
    "}\n",
    "\n",
    "print(f\"Computing distances for {len(random_pairs)} random sentence pairs...\")\n",
    "for s1, s2 in tqdm(random_pairs):\n",
    "    emb1 = model.encode(s1, convert_to_tensor=True)\n",
    "    emb2 = model.encode(s2, convert_to_tensor=True)\n",
    "    \n",
    "    bg_distances['euclidean'].append(distance.euclidean(emb1.cpu().numpy(), emb2.cpu().numpy()))\n",
    "    bg_distances['manhattan'].append(distance.cityblock(emb1.cpu().numpy(), emb2.cpu().numpy()))\n",
    "    bg_distances['dot_product'].append(torch.dot(emb1, emb2).item())\n",
    "    bg_distances['angular'].append(acos(1 - distance.cosine(emb1.cpu().numpy(), emb2.cpu().numpy()))/pi)\n",
    "\n",
    "# Compute statistics for each metric\n",
    "bg_stats = {}\n",
    "for metric in bg_distances.keys():\n",
    "    bg_stats[metric] = {\n",
    "        'mean': np.mean(bg_distances[metric]),\n",
    "        'std': np.std(bg_distances[metric]),\n",
    "        'min': np.min(bg_distances[metric]),\n",
    "        'max': np.max(bg_distances[metric])\n",
    "    }\n",
    "\n",
    "print(\"\\nBackground distribution statistics:\")\n",
    "pd.DataFrame(bg_stats).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75bdbb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution-based calibrations:\n",
      "- background: 0 = most similar in background, 1 = most dissimilar in background\n",
      "- percentile: % of background pairs that are more similar than this pair\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>euclidean_background</th>\n",
       "      <th>euclidean_percentile</th>\n",
       "      <th>manhattan_background</th>\n",
       "      <th>manhattan_percentile</th>\n",
       "      <th>angular_background</th>\n",
       "      <th>angular_percentile</th>\n",
       "      <th>dot_product_background</th>\n",
       "      <th>dot_product_percentile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.541703</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.542567</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.504191</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.375968</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.24897</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.241088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.216255</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-0.041381</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.463052</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.457024</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.580192</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.297067</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.94342</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0.934279</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.070815</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.907785</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  euclidean_background  euclidean_percentile manhattan_background  \\\n",
       "0             0.541703                   2.0             0.542567   \n",
       "1             -0.24897                   0.0            -0.241088   \n",
       "2             0.463052                   2.0             0.457024   \n",
       "3              0.94342                  88.0             0.934279   \n",
       "\n",
       "   manhattan_percentile  angular_background  angular_percentile  \\\n",
       "0                   2.0            0.504191                98.0   \n",
       "1                   0.0            1.216255               100.0   \n",
       "2                   2.0            0.580192                98.0   \n",
       "3                  90.0            0.070815                12.0   \n",
       "\n",
       "   dot_product_background  dot_product_percentile  \n",
       "0                0.375968                     2.0  \n",
       "1               -0.041381                     0.0  \n",
       "2                0.297067                     2.0  \n",
       "3                0.907785                    88.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create distribution-based calibrations using min-max normalization\n",
    "distances_distribution = pd.DataFrame(index=distances.index)\n",
    "\n",
    "for metric in ['euclidean', 'manhattan', 'angular', 'dot_product']:\n",
    "    bg_vals = np.array(bg_distances[metric])\n",
    "    \n",
    "    # Min-max normalization based on background distribution\n",
    "    # For dot_product, invert because higher = more similar (opposite of distance)\n",
    "    if metric == 'dot_product':\n",
    "        min_val = bg_vals.min()\n",
    "        max_val = bg_vals.max()\n",
    "        # Invert: (max - value) / (max - min) so higher similarity = lower distance\n",
    "        distances_distribution[f'{metric}_background'] = (max_val - distances[metric]) / (max_val - min_val)\n",
    "    else:\n",
    "        min_val = bg_vals.min()\n",
    "        max_val = bg_vals.max()\n",
    "        # Standard min-max: (value - min) / (max - min)\n",
    "        distances_distribution[f'{metric}_background'] = (distances[metric] - min_val) / (max_val - min_val)\n",
    "    \n",
    "    # Percentile calibration - what % of background values indicate more distance\n",
    "    if metric == 'dot_product':\n",
    "        # For dot product, higher = more similar, so count values > current (more similar)\n",
    "        distances_distribution[f'{metric}_percentile'] = [\n",
    "            (np.sum(bg_vals > val) / len(bg_vals)) * 100 for val in distances[metric]\n",
    "        ]\n",
    "    else:\n",
    "        # For distances, count values < current (less distant)\n",
    "        distances_distribution[f'{metric}_percentile'] = [\n",
    "            (np.sum(bg_vals < val) / len(bg_vals)) * 100 for val in distances[metric]\n",
    "        ]\n",
    "\n",
    "print(\"Distribution-based calibrations:\")\n",
    "print(\"- background: 0 = most similar in background, 1 = most dissimilar in background\")\n",
    "print(\"- percentile: % of background pairs that are more similar than this pair\")\n",
    "distances_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6152e765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>euclidean</th>\n",
       "      <th>euclidean_norm</th>\n",
       "      <th>euclidean_background</th>\n",
       "      <th>manhattan</th>\n",
       "      <th>manhattan_norm</th>\n",
       "      <th>manhattan_background</th>\n",
       "      <th>dot_product</th>\n",
       "      <th>dot_product_norm</th>\n",
       "      <th>dot_product_background</th>\n",
       "      <th>angular</th>\n",
       "      <th>angular_norm</th>\n",
       "      <th>angular_background</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.935487</td>\n",
       "      <td>0.688206</td>\n",
       "      <td>0.541703</td>\n",
       "      <td>14.680563</td>\n",
       "      <td>0.704347</td>\n",
       "      <td>0.542567</td>\n",
       "      <td>0.562432</td>\n",
       "      <td>4.736273e-01</td>\n",
       "      <td>0.375968</td>\n",
       "      <td>0.690134</td>\n",
       "      <td>0.651331</td>\n",
       "      <td>0.504191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.24897</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.241088</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.290332e-07</td>\n",
       "      <td>-0.041381</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>1.216255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.842431</td>\n",
       "      <td>0.619748</td>\n",
       "      <td>0.463052</td>\n",
       "      <td>13.078041</td>\n",
       "      <td>0.62746</td>\n",
       "      <td>0.457024</td>\n",
       "      <td>0.645155</td>\n",
       "      <td>3.840870e-01</td>\n",
       "      <td>0.297067</td>\n",
       "      <td>0.723207</td>\n",
       "      <td>0.581812</td>\n",
       "      <td>0.580192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.41078</td>\n",
       "      <td>1.037863</td>\n",
       "      <td>0.94342</td>\n",
       "      <td>22.018671</td>\n",
       "      <td>1.056415</td>\n",
       "      <td>0.934279</td>\n",
       "      <td>0.004850</td>\n",
       "      <td>1.077159e+00</td>\n",
       "      <td>0.907785</td>\n",
       "      <td>0.501544</td>\n",
       "      <td>1.047745</td>\n",
       "      <td>0.070815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  euclidean euclidean_norm euclidean_background  manhattan manhattan_norm  \\\n",
       "0  0.935487       0.688206             0.541703  14.680563       0.704347   \n",
       "1       0.0            0.0             -0.24897        0.0            0.0   \n",
       "2  0.842431       0.619748             0.463052  13.078041        0.62746   \n",
       "3   1.41078       1.037863              0.94342  22.018671       1.056415   \n",
       "\n",
       "  manhattan_background  dot_product  dot_product_norm  dot_product_background  \\\n",
       "0             0.542567     0.562432      4.736273e-01                0.375968   \n",
       "1            -0.241088     1.000000     -1.290332e-07               -0.041381   \n",
       "2             0.457024     0.645155      3.840870e-01                0.297067   \n",
       "3             0.934279     0.004850      1.077159e+00                0.907785   \n",
       "\n",
       "    angular  angular_norm  angular_background  \n",
       "0  0.690134      0.651331            0.504191  \n",
       "1  1.000000     -0.000000            1.216255  \n",
       "2  0.723207      0.581812            0.580192  \n",
       "3  0.501544      1.047745            0.070815  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine all three approaches: raw, anchor-normalized, and distribution-calibrated\n",
    "combined_all = pd.concat([distances, distances_norm, distances_distribution], axis=1)\n",
    "\n",
    "# Reorder columns to group each metric with its calibrations\n",
    "column_order = []\n",
    "for metric in ['euclidean', 'manhattan', 'dot_product', 'angular']:\n",
    "    column_order.extend([\n",
    "        metric,                   # raw\n",
    "        f'{metric}_norm',         # anchor-normalized\n",
    "        f'{metric}_background'    # background min-max normalized\n",
    "    ])\n",
    "\n",
    "combined_all = combined_all[column_order]\n",
    "combined_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3798d466",
   "metadata": {},
   "source": [
    "You can see that the background distribution calibration offers another way to ground the distances in a more global context, rather than relying on a few manually chosen anchors. This is more statistically principled, but also depends on having a representative background corpus. In this case, there are not more extreme values in the background distribution than in my target set, so the normalization is less effective at spreading out the scores. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
