{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eca017c",
   "metadata": {},
   "source": [
    "# Sentence Embeddings + Other Comparisons\n",
    "### Authored by Hilary Bayer\n",
    "\n",
    "**Overview**\n",
    "Like the sentence embeddings and cosine similarity approach we tested, this approach first requires embedding the test phrases. Embeddings are numeric vector representations of each phrase and are designed to capture the semantic meaning of the phrases as opposed to the superficial similarity of letters or other elements as we did in the token overlap approach. We use the SentenceTransformer Python module for this task.<br>\n",
    "\n",
    "Next, we compare the embeddings to assess the similarity of the topics discussed in each of the phrases. In the second approach, we used cosine similarity to compare the embeddings. In this approach, for each pair of embeddings we calculate the Euclidean distance, Manhattan distance, and angular distance using `scipy`. We also find the dot product with `torch.dot`.<br>\n",
    "\n",
    "**Comparison**<br>\n",
    "The dot product $\\mathbf{A} \\cdot \\mathbf{B}$ is simplest metric but it is important to note that in contrast to the distance measures it increases with similarity of the phrases rather than with dissimilarity. It reflects both the magnitude and direction of the vectors.\n",
    "\n",
    "The Euclidean distance for embeddings $A = (a_1, a_2, \\dots, a_n)$ and $B = (b_1, b_2, \\dots, b_n)$ is $D_{\\text{Euclidean}}(A, B) = \\sqrt{\\sum_{i=1}^{n} (a_i - b_i)^2}$. It is a straightforward and common distance metric that is affected by the magnitude of the embedding vectors. \n",
    "\n",
    "The Manhattan distance for embeddings $A = (a_1, a_2, \\dots, a_n)$ and $B = (b_1, b_2, \\dots, b_n)$ is \n",
    "$D_{\\text{Manhattan}}(A, B) = \\sum_{i=1}^{n} |a_i - b_i|$. It is less influenced by outliers than the Euclidean distance and is simple to compute. It is also sensitive to the magnitude of the embedding vectors. \n",
    "\n",
    "The angular distance for embeddings $A$ and $B$ is $D_{\\text{angular}}(A, B) = \\frac{ \\arccos \\left( \\frac{ A \\cdot B }{ \\|A\\| \\, \\|B\\| } \\right) }{\\pi}$. This measure is calculated from cosine similarity, though it is a distance not a similarity. It is the most complicated to compute and least influenced by vector magnitude of the distance metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca07c88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from scipy.spatial import distance\n",
    "from math import acos, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54d7c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load evaluation phrase pairs\n",
    "df = pd.read_csv('evaluation_cases.csv')\n",
    "\n",
    "# load model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# create dataframe to store distances\n",
    "distances = pd.DataFrame(columns=['euclidean', 'manhattan', 'dot_product', 'angular'])\n",
    "\n",
    "# loop through the evaluation cases\n",
    "for index, row in df.iterrows():\n",
    "    text_a = row['sent1']\n",
    "    text_b = row['sent2']\n",
    "\n",
    "    # encode sentences\n",
    "    emb1 = model.encode(text_a, convert_to_tensor=True)\n",
    "    emb2 = model.encode(text_b, convert_to_tensor=True)\n",
    "\n",
    "    # compute distances\n",
    "    distances.at[index, 'euclidean'] = distance.euclidean(emb1.squeeze().cpu().numpy(), emb2.cpu().numpy())\n",
    "    distances.at[index, 'manhattan'] = distance.cityblock(emb1.squeeze().cpu().numpy(), emb2.cpu().numpy()) \n",
    "    distances.at[index, 'dot_product'] = torch.dot(emb1, emb2).item()\n",
    "    distances.at[index, 'angular'] = acos(1 - distance.cosine(emb1.cpu().numpy(), emb2.cpu().numpy()))/pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70c2612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize distances \n",
    "distances['euclidean_normalized'] = distances['euclidean'] / distances['euclidean'].sum()\n",
    "distances['manhattan_normalized'] = distances['manhattan'] / distances['manhattan'].sum()\n",
    "distances['dot_product_normalized'] = distances['dot_product'] / distances['dot_product'].sum()\n",
    "distances['angular_normalized'] = distances['angular'] / distances['angular'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23c2056d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>euclidean</th>\n",
       "      <th>manhattan</th>\n",
       "      <th>dot_product</th>\n",
       "      <th>angular</th>\n",
       "      <th>euclidean_normalized</th>\n",
       "      <th>manhattan_normalized</th>\n",
       "      <th>dot_product_normalized</th>\n",
       "      <th>angular_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.935487</td>\n",
       "      <td>14.680565</td>\n",
       "      <td>0.562432</td>\n",
       "      <td>0.309866</td>\n",
       "      <td>0.293376</td>\n",
       "      <td>0.294925</td>\n",
       "      <td>0.254214</td>\n",
       "      <td>0.28556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.45199</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.842431</td>\n",
       "      <td>13.078042</td>\n",
       "      <td>0.645155</td>\n",
       "      <td>0.276793</td>\n",
       "      <td>0.264193</td>\n",
       "      <td>0.262731</td>\n",
       "      <td>0.291604</td>\n",
       "      <td>0.255082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.41078</td>\n",
       "      <td>22.018671</td>\n",
       "      <td>0.00485</td>\n",
       "      <td>0.498456</td>\n",
       "      <td>0.442431</td>\n",
       "      <td>0.442344</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.459358</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  euclidean  manhattan dot_product   angular euclidean_normalized  \\\n",
       "0  0.935487  14.680565    0.562432  0.309866             0.293376   \n",
       "1       0.0        0.0         1.0       0.0                  0.0   \n",
       "2  0.842431  13.078042    0.645155  0.276793             0.264193   \n",
       "3   1.41078  22.018671     0.00485  0.498456             0.442431   \n",
       "\n",
       "  manhattan_normalized dot_product_normalized angular_normalized  \n",
       "0             0.294925               0.254214            0.28556  \n",
       "1                  0.0                0.45199                0.0  \n",
       "2             0.262731               0.291604           0.255082  \n",
       "3             0.442344               0.002192           0.459358  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7ba9b8",
   "metadata": {},
   "source": [
    "**Results**<br>\n",
    "All three distance measures agree that in order of increasing distance the phrase pairs are:\n",
    "- \"he ran quickly to the store\" and \"he ran quickly to the store\"\n",
    "- \"domestic unrest\" and\t\"political instability in the country\"\n",
    "- \"the cat sat on the mat\" and \"a feline rested atop a rug\"\n",
    "- \"turn left at the traffic light\" and \"photosynthesis occurs in plant cells\"\n",
    "\n",
    "Ranking these phrase pairs in order of decreasing dot product produces the same results. \n",
    "\n",
    "These rankings differ from the reference rankings which rank the cat and feline pair of phrases as more similar than the domestic unrest and political instability pair of phrases. \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topicbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
